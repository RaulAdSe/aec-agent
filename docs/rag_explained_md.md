# ğŸ” RAG (Retrieval Augmented Generation) - ExplicaciÃ³n TÃ©cnica

## ğŸ“‹ Tabla de Contenidos

1. [Â¿QuÃ© es RAG?](#quÃ©-es-rag)
2. [El Problema que Resuelve](#el-problema-que-resuelve)
3. [Arquitectura de RAG](#arquitectura-de-rag)
4. [Componentes Clave](#componentes-clave)
5. [Pipeline Completo](#pipeline-completo)
6. [ImplementaciÃ³n con LangChain](#implementaciÃ³n-con-langchain)
7. [Embeddings](#embeddings)
8. [Vectorstores](#vectorstores)
9. [Retrieval Strategies](#retrieval-strategies)
10. [Best Practices](#best-practices)
11. [Debugging & Optimization](#debugging--optimization)

---

## Â¿QuÃ© es RAG?

**RAG = Retrieval Augmented Generation**

Framework que permite a los LLMs acceder a informaciÃ³n externa actualizada mediante:
- ğŸ“š **Retrieval**: Buscar documentos relevantes
- ğŸ§  **Augmentation**: AÃ±adir contexto al prompt
- âœï¸ **Generation**: Generar respuesta informada

### AnalogÃ­a Simple

Imagina que eres un estudiante haciendo un examen:
- **Sin RAG**: Solo puedes usar lo que memorizaste
- **Con RAG**: Puedes consultar tus apuntes durante el examen

---

## El Problema que Resuelve

### LLMs Tradicionales: Limitaciones

#### Problema 1: Conocimiento EstÃ¡tico

```python
# LLM sin RAG
pregunta = "Â¿QuÃ© dice el CTE DB-SI actualizado en 2023 sobre puertas de evacuaciÃ³n?"
respuesta = llm(pregunta)
# âŒ Responde con conocimiento antiguo o incorrecto
```

#### Problema 2: Alucinaciones

```python
# Sin RAG, el LLM puede inventar informaciÃ³n
pregunta = "Â¿CuÃ¡l es el ancho mÃ­nimo de puerta segÃºn el CTE?"
respuesta = llm(pregunta)
# âŒ "El ancho mÃ­nimo es 75cm" (inventado)
```

#### Problema 3: Sin Citas

```python
# No puedes verificar la fuente
respuesta = "El ancho mÃ­nimo es 80cm"
# â“ Â¿De dÃ³nde sale este dato?
```

### RAG: La SoluciÃ³n

```python
# Con RAG
pregunta = "Â¿QuÃ© dice el CTE DB-SI sobre puertas de evacuaciÃ³n?"

# 1. Buscar en la base de conocimiento
chunks_relevantes = vectorstore.similarity_search(pregunta)
# Encuentra: "CTE DB-SI, SecciÃ³n 4.2: El ancho mÃ­nimo de puertas de evacuaciÃ³n..."

# 2. AÃ±adir contexto al prompt
contexto = "\n".join(chunks_relevantes)
prompt = f"Contexto: {contexto}\n\nPregunta: {pregunta}"

# 3. Generar respuesta informada
respuesta = llm(prompt)
# âœ… "SegÃºn el CTE DB-SI SecciÃ³n 4.2, el ancho mÃ­nimo es 80cm [Fuente: CTE_DB-SI.pdf, pÃ¡g. 23]"
```

---

## Arquitectura de RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FASE 1: INDEXACIÃ“N                       â”‚
â”‚                    (Se hace una vez)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“„ Documentos (PDFs, TXT, etc.)
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Loader    â”‚  â† Carga documentos
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Splitter   â”‚  â† Divide en chunks
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Embeddings  â”‚  â† Convierte a vectores
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vectorstore â”‚  â† Almacena vectores
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 2: CONSULTA                          â”‚
â”‚                  (Cada vez que preguntas)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ’¬ Pregunta del usuario
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Embeddings  â”‚  â† Convierte pregunta a vector
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Retriever  â”‚  â† Busca chunks similares
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Prompt    â”‚  â† Construye prompt con contexto
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     LLM     â”‚  â† Genera respuesta
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    âœ… Respuesta + Citas
```

---

## Componentes Clave

### 1. Document Loader

Carga documentos de diferentes fuentes:

```python
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# PDF
loader = PyPDFLoader("CTE_DB-SI.pdf")
docs = loader.load()

# Texto plano
loader = TextLoader("normativa.txt")
docs = loader.load()
```

**Cada documento tiene**:
- `page_content`: El texto
- `metadata`: Info adicional (nombre archivo, pÃ¡gina, etc.)

### 2. Text Splitter

Divide documentos en chunks manejables:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # TamaÃ±o de cada chunk (caracteres)
    chunk_overlap=200,      # Solapamiento entre chunks
    separators=["\n\n", "\n", " ", ""]  # Prioridad de separaciÃ³n
)

chunks = splitter.split_documents(docs)
```

**Â¿Por quÃ© dividir?**
- Los LLMs tienen lÃ­mite de tokens
- Chunks pequeÃ±os = retrieval mÃ¡s preciso
- Overlap = preservar contexto entre chunks

#### Ejemplo de Splitting

```
Documento original:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"El CTE DB-SI establece que las puertas de evacuaciÃ³n
deben tener un ancho mÃ­nimo de 80cm. En caso de edificios
de pÃºblica concurrencia, este ancho se incrementa a 1.20m.
Las puertas deben abrirse en el sentido de la evacuaciÃ³n..."
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DespuÃ©s del splitting (chunk_size=100, overlap=20):

Chunk 1:
"El CTE DB-SI establece que las puertas de evacuaciÃ³n
deben tener un ancho mÃ­nimo de 80cm."

Chunk 2 (con overlap):
"ancho mÃ­nimo de 80cm. En caso de edificios de pÃºblica
concurrencia, este ancho se incrementa a 1.20m."

Chunk 3:
"ancho se incrementa a 1.20m. Las puertas deben abrirse
en el sentido de la evacuaciÃ³n..."
```

### 3. Embeddings

Convierte texto a vectores numÃ©ricos:

```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Convertir texto a vector
vector = embeddings.embed_query("Â¿Ancho mÃ­nimo de puerta?")
# Output: [0.12, -0.45, 0.89, ..., 0.34]  (384 dimensiones)
```

**Propiedad clave**: Textos similares â†’ Vectores cercanos

```python
v1 = embeddings.embed_query("ancho de puerta")
v2 = embeddings.embed_query("anchura de entrada")
v3 = embeddings.embed_query("precio del tomate")

# Distancia coseno:
# similarity(v1, v2) = 0.89  â† Muy similar
# similarity(v1, v3) = 0.12  â† Poco similar
```

### 4. Vectorstore

Almacena y busca vectores eficientemente:

```python
from langchain_community.vectorstores import Chroma

# Crear vectorstore
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./vectorstore"
)

# Buscar documentos similares
results = vectorstore.similarity_search(
    "Â¿Ancho mÃ­nimo de puerta?",
    k=3  # Top 3 resultados
)
```

**Â¿CÃ³mo funciona?**
1. Convierte la pregunta a vector
2. Calcula distancia a todos los vectores almacenados
3. Devuelve los K mÃ¡s cercanos

### 5. Retriever

Interfaz para recuperar documentos:

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",  # O "mmr" para diversidad
    search_kwargs={"k": 3}
)

docs = retriever.get_relevant_documents("Â¿Ancho mÃ­nimo de puerta?")
```

### 6. LLM

Genera la respuesta final:

```python
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-pro",
    temperature=0.1  # Bajo = mÃ¡s determinista
)
```

### 7. Prompt Template

Estructura el prompt con contexto:

```python
from langchain.prompts import ChatPromptTemplate

template = """
Eres un asistente experto en normativa de construcciÃ³n.
Responde la pregunta basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado.
Si no puedes responder con el contexto dado, di "No tengo informaciÃ³n suficiente".

Contexto:
{context}

Pregunta: {question}

Respuesta:
"""

prompt = ChatPromptTemplate.from_template(template)
```

---

## Pipeline Completo

### ImplementaciÃ³n Paso a Paso

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

# 1. Cargar documentos
loader = PyPDFLoader("CTE_DB-SI.pdf")
documents = loader.load()

# 2. Dividir en chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# 3. Crear embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# 4. Crear vectorstore
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./vectorstore"
)

# 5. Crear retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 6. Crear LLM
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.1)

# 7. Crear cadena QA
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# 8. Hacer pregunta
query = "Â¿CuÃ¡l es el ancho mÃ­nimo de una puerta de evacuaciÃ³n?"
result = qa_chain({"query": query})

print(result["result"])  # Respuesta
print(result["source_documents"])  # Fuentes
```

---

## ImplementaciÃ³n con LangChain

### Estructura del Proyecto

```
src/rag/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ document_loader.py      # Carga PDFs
â”œâ”€â”€ embeddings_config.py    # ConfiguraciÃ³n de embeddings
â”œâ”€â”€ vectorstore_manager.py  # GestiÃ³n de vectorstore
â””â”€â”€ qa_chain.py            # Cadena de QA
```

### document_loader.py

```python
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from typing import List
from langchain.schema import Document

def load_pdfs(pdf_dir: Path) -> List[Document]:
    """
    Carga todos los PDFs de un directorio.
    
    Args:
        pdf_dir: Directorio con PDFs
        
    Returns:
        Lista de documentos cargados
    """
    documents = []
    
    for pdf_path in pdf_dir.glob("*.pdf"):
        print(f"Cargando: {pdf_path.name}")
        loader = PyPDFLoader(str(pdf_path))
        docs = loader.load()
        documents.extend(docs)
    
    print(f"Total documentos cargados: {len(documents)}")
    return documents
```

### embeddings_config.py

```python
from langchain_huggingface import HuggingFaceEmbeddings

def get_embeddings():
    """
    Crea instancia de embeddings.
    Usando modelo multilingual para soportar espaÃ±ol.
    """
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},  # O 'cuda' si tienes GPU
        encode_kwargs={'normalize_embeddings': True}  # Normalizar vectores
    )
```

### vectorstore_manager.py

```python
from pathlib import Path
from typing import List, Optional
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from .embeddings_config import get_embeddings
from .document_loader import load_pdfs

class VectorstoreManager:
    """Gestiona la creaciÃ³n y carga del vectorstore."""
    
    def __init__(self, persist_directory: Path):
        self.persist_directory = persist_directory
        self.embeddings = get_embeddings()
        self.vectorstore: Optional[Chroma] = None
    
    def create_from_pdfs(self, pdf_dir: Path, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        Crea vectorstore desde PDFs.
        
        Args:
            pdf_dir: Directorio con PDFs
            chunk_size: TamaÃ±o de cada chunk
            chunk_overlap: Solapamiento entre chunks
        """
        # 1. Cargar documentos
        documents = load_pdfs(pdf_dir)
        
        # 2. Dividir en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        print(f"Total chunks: {len(chunks)}")
        
        # 3. Crear vectorstore
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=str(self.persist_directory)
        )
        
        print(f"Vectorstore creado en: {self.persist_directory}")
    
    def load_existing(self):
        """Carga vectorstore existente."""
        self.vectorstore = Chroma(
            persist_directory=str(self.persist_directory),
            embedding_function=self.embeddings
        )
        print(f"Vectorstore cargado desde: {self.persist_directory}")
    
    def get_retriever(self, k: int = 3):
        """
        Obtiene retriever para bÃºsquedas.
        
        Args:
            k: NÃºmero de documentos a recuperar
        """
        if not self.vectorstore:
            raise ValueError("Vectorstore no inicializado. Usa create_from_pdfs() o load_existing()")
        
        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )
```

### qa_chain.py

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

SYSTEM_PROMPT = """
Eres un asistente experto en normativa de construcciÃ³n espaÃ±ola (CTE, CTE DB-SI, CTE DB-SUA).
Tu trabajo es responder preguntas basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado.

Reglas:
1. Si la informaciÃ³n estÃ¡ en el contexto, responde de forma clara y precisa
2. Siempre cita la fuente (nombre del documento, secciÃ³n, pÃ¡gina si estÃ¡ disponible)
3. Si no puedes responder con el contexto dado, di "No tengo informaciÃ³n suficiente sobre esto en la normativa proporcionada"
4. No inventes ni supongas informaciÃ³n

Contexto:
{context}

Pregunta: {question}

Respuesta detallada:
"""

def create_qa_chain(retriever, model_name: str = "gemini-pro", temperature: float = 0.1):
    """
    Crea cadena de QA con retrieval.
    
    Args:
        retriever: Retriever del vectorstore
        model_name: Nombre del modelo de Google
        temperature: Temperatura del LLM
    
    Returns:
        Cadena de QA configurada
    """
    # LLM
    llm = ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature
    )
    
    # Prompt
    prompt = PromptTemplate(
        template=SYSTEM_PROMPT,
        input_variables=["context", "question"]
    )
    
    # Cadena
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # Estrategia: meter todo el contexto
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    
    return qa_chain

def query(qa_chain, question: str):
    """
    Hace una pregunta a la cadena de QA.
    
    Args:
        qa_chain: Cadena de QA
        question: Pregunta del usuario
    
    Returns:
        Dict con 'result' y 'source_documents'
    """
    result = qa_chain({"query": question})
    return result
```

---

## Embeddings

### Â¿QuÃ© son?

Representaciones numÃ©ricas de texto que capturan significado semÃ¡ntico.

```
"ancho de puerta" â†’ [0.12, -0.45, 0.89, ..., 0.34]
                     â†‘
                  Vector de 384 dimensiones
```

### Tipos de Embeddings

#### 1. Word Embeddings (Antiguo)

Cada palabra tiene un vector fijo:

```python
# Word2Vec
"puerta" â†’ [0.1, 0.5, ...]
"door" â†’ [0.2, 0.6, ...]
```

**Problema**: "banco" (asiento) y "banco" (instituciÃ³n) tienen el mismo vector.

#### 2. Sentence Embeddings (Moderno)

Toda la frase tiene un vector:

```python
"ancho de puerta de evacuaciÃ³n" â†’ [0.12, -0.45, ..., 0.34]
"width of evacuation door" â†’ [0.15, -0.42, ..., 0.31]  # Similar
```

**Ventaja**: Captura contexto completo.

### Modelos Recomendados

#### Para EspaÃ±ol/MultilingÃ¼e

```python
# OpciÃ³n 1: PequeÃ±o y rÃ¡pido (recomendado)
"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# - TamaÃ±o: 420 MB
# - Dimensiones: 384
# - Idiomas: 50+

# OpciÃ³n 2: MÃ¡s preciso pero mÃ¡s pesado
"sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# - TamaÃ±o: 1.1 GB
# - Dimensiones: 768
# - Idiomas: 50+
```

#### Para Solo InglÃ©s

```python
# Mejor rendimiento
"sentence-transformers/all-MiniLM-L6-v2"
# - TamaÃ±o: 80 MB
# - Dimensiones: 384
```

### Uso en CÃ³digo

```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Embeddings para una query
query_vector = embeddings.embed_query("Â¿Ancho mÃ­nimo de puerta?")
print(f"Dimensiones: {len(query_vector)}")  # 384

# Embeddings para mÃºltiples documentos
docs = ["doc1", "doc2", "doc3"]
doc_vectors = embeddings.embed_documents(docs)
print(f"NÃºmero de vectores: {len(doc_vectors)}")  # 3
```

---

## Vectorstores

### Â¿QuÃ© es un Vectorstore?

Base de datos especializada en:
1. Almacenar vectores de alta dimensionalidad
2. Buscar vectores similares eficientemente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         VECTORSTORE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ID  â”‚ Vector          â”‚ Metadata      â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1   â”‚ [0.1, 0.5, ...] â”‚ {page: 1,...} â”‚
â”‚ 2   â”‚ [0.2, 0.6, ...] â”‚ {page: 2,...} â”‚
â”‚ 3   â”‚ [0.3, 0.4, ...] â”‚ {page: 3,...} â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Opciones Populares

#### ChromaDB (Recomendado para POC)

```python
from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./vectorstore"  # Persiste en disco
)
```

**Pros**:
- âœ… Simple de usar
- âœ… Funciona en local (no requiere servidor)
- âœ… Persiste automÃ¡ticamente
- âœ… Bueno para desarrollo

**Cons**:
- âŒ No escalable para producciÃ³n (millones de vectores)

#### Pinecone (ProducciÃ³n)

```python
from langchain_community.vectorstores import Pinecone

vectorstore = Pinecone.from_documents(
    documents=chunks,
    embedding=embeddings,
    index_name="normativa"
)
```

**Pros**:
- âœ… Muy escalable
- âœ… Servicio gestionado
- âœ… RÃ¡pido en bÃºsquedas

**Cons**:
- âŒ Requiere cuenta y pago
- âŒ MÃ¡s complejo de configurar

#### FAISS (Local, Escalable)

```python
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)
vectorstore.save_local("./vectorstore")
```

**Pros**:
- âœ… Muy rÃ¡pido
- âœ… Funciona en local
- âœ… Escalable a millones de vectores

**Cons**:
- âŒ MÃ¡s complejo que ChromaDB
- âŒ Requiere mÃ¡s memoria RAM

### BÃºsqueda de Similitud

#### Distancia Coseno (Por Defecto)

Mide el Ã¡ngulo entre vectores:

```python
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(v1, v2):
    return dot(v1, v2) / (norm(v1) * norm(v2))

# Rango: [-1, 1]
# 1 = idÃ©nticos
# 0 = ortogonales
# -1 = opuestos
```

#### Distancia Euclidiana

Mide la distancia "real" entre vectores:

```python
from numpy import sqrt, sum

def euclidean_distance(v1, v2):
    return sqrt(sum((v1 - v2) ** 2))

# Rango: [0, âˆ)
# 0 = idÃ©nticos
# Mayor valor = mÃ¡s diferentes
```

---

## Retrieval Strategies

### 1. Similarity Search (Por Defecto)

Busca los K vectores mÃ¡s similares:

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

docs = retriever.get_relevant_documents("Â¿Ancho mÃ­nimo de puerta?")
# Devuelve los 3 chunks mÃ¡s similares
```

### 2. MMR (Maximal Marginal Relevance)

Balancea relevancia con diversidad:

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 3,
        "fetch_k": 20,  # Candidatos a considerar
        "lambda_mult": 0.5  # 0 = diversidad, 1 = relevancia
    }
)
```

**Â¿CuÃ¡ndo usar MMR?**
- âœ… Cuando quieres perspectivas diferentes
- âœ… Para evitar chunks muy repetitivos
- âŒ No uses si necesitas mÃ¡xima precisiÃ³n

### 3. Threshold-based

Solo devuelve resultados por encima de cierto score:

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.8,  # MÃ­nimo 80% de similitud
        "k": 5
    }
)
```

---

## Best Practices

### 1. Chunk Size Ã“ptimo

```python
# Para documentaciÃ³n tÃ©cnica
chunk_size = 1000  # 1000 caracteres â‰ˆ 250 tokens
chunk_overlap = 200  # 20% overlap

# Para preguntas cortas
chunk_size = 500
chunk_overlap = 100

# Para contextos largos
chunk_size = 1500
chunk_overlap = 300
```

**Regla general**: 
- Chunks pequeÃ±os â†’ BÃºsqueda precisa, pero puede perder contexto
- Chunks grandes â†’ MÃ¡s contexto, pero menos preciso

### 2. Metadata

AÃ±ade metadata Ãºtil a cada chunk:

```python
from langchain.schema import Document

doc = Document(
    page_content="El ancho mÃ­nimo es 80cm",
    metadata={
        "source": "CTE_DB-SI.pdf",
        "page": 23,
        "section": "4.2 Puertas de evacuaciÃ³n",
        "chapter": "EvacuaciÃ³n"
    }
)
```

Luego puedes filtrar por metadata:

```python
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 3,
        "filter": {"section": "4.2 Puertas de evacuaciÃ³n"}
    }
)
```

### 3. Prompt Engineering

**Mal prompt**:
```python
"Responde la pregunta: {question}"
```

**Buen prompt**:
```python
"""
Eres un experto en normativa de construcciÃ³n.
Responde basÃ¡ndote ÃšNICAMENTE en el contexto.
Cita siempre la fuente.

Contexto: {context}
Pregunta: {question}
Respuesta:
"""
```

### 4. Temperatura del LLM

```python
# Para respuestas tÃ©cnicas/precisas
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.1)

# Para respuestas creativas
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.7)
```

### 5. EvaluaciÃ³n

Mide la calidad de tu RAG:

```python
from langchain.evaluation.qa import QAEvalChain

# Pares de pregunta-respuesta esperada
examples = [
    {
        "query": "Â¿Ancho mÃ­nimo de puerta?",
        "answer": "80cm segÃºn CTE DB-SI"
    },
    # ...
]

# Evaluar
predictions = [qa_chain(ex["query"]) for ex in examples]
eval_chain = QAEvalChain.from_llm(llm)
graded_outputs = eval_chain.evaluate(examples, predictions)
```

---

## Debugging & Optimization

### Problema 1: Respuestas Incorrectas

**DiagnÃ³stico**:
```python
result = qa_chain({"query": "Â¿Ancho mÃ­nimo de puerta?"})

# 1. Revisar chunks recuperados
for doc in result["source_documents"]:
    print(doc.page_content)
    print(doc.metadata)
    print("---")
```

**Posibles causas**:
- âŒ Chunks no contienen la informaciÃ³n
- âŒ K demasiado bajo (aumenta a 5-7)
- âŒ Embeddings no son buenos para espaÃ±ol

**SoluciÃ³n**:
```python
# Aumentar K
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Cambiar modelo de embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"  # Mejor
)
```

### Problema 2: Contexto Insuficiente

**SoluciÃ³n**: Aumentar chunk_size y overlap

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # Era 1000
    chunk_overlap=300  # Era 200
)
```

### Problema 3: Lento

**DiagnÃ³stico**:
```python
import time

start = time.time()
result = qa_chain({"query": "pregunta"})
print(f"Tiempo: {time.time() - start:.2f}s")
```

**Optimizaciones**:

1. **Usar GPU para embeddings**:
```python
embeddings = HuggingFaceEmbeddings(
    model_kwargs={'device': 'cuda'}  # Era 'cpu'
)
```

2. **Reducir K**:
```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})  # Era 5
```

3. **Usar modelo de embeddings mÃ¡s pequeÃ±o**:
```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"  # MÃ¡s rÃ¡pido
)
```

### Problema 4: Vectorstore Muy Grande

**SoluciÃ³n**: Limpieza y optimizaciÃ³n

```python
# Solo indexar PDFs relevantes
pdfs_to_index = [
    "CTE_DB-SI.pdf",
    "CTE_DB-SUA.pdf"
]

documents = []
for pdf in pdfs_to_index:
    loader = PyPDFLoader(pdf)
    documents.extend(loader.load())
```

---

## Ejemplo Completo

```python
# main.py
from pathlib import Path
from src.rag.vectorstore_manager import VectorstoreManager
from src.rag.qa_chain import create_qa_chain, query

def main():
    # ConfiguraciÃ³n
    PDF_DIR = Path("data/normativa")
    VECTORSTORE_DIR = Path("vectorstore/normativa_db")
    
    # Crear o cargar vectorstore
    rag = VectorstoreManager(VECTORSTORE_DIR)
    
    if not VECTORSTORE_DIR.exists():
        print("Creando vectorstore por primera vez...")
        rag.create_from_pdfs(PDF_DIR)
    else:
        print("Cargando vectorstore existente...")
        rag.load_existing()
    
    # Crear retriever
    retriever = rag.get_retriever(k=3)
    
    # Crear cadena de QA
    qa_chain = create_qa_chain(retriever)
    
    # Hacer preguntas
    questions = [
        "Â¿CuÃ¡l es el ancho mÃ­nimo de una puerta de evacuaciÃ³n?",
        "Â¿QuÃ© dice el CTE sobre distancias mÃ¡ximas de evacuaciÃ³n?",
        "Â¿Requisitos de resistencia al fuego para muros?"
    ]
    
    for question in questions:
        print(f"\nPregunta: {question}")
        result = query(qa_chain, question)
        print(f"Respuesta: {result['result']}")
        print("\nFuentes:")
        for doc in result['source_documents']:
            print(f"  - {doc.metadata.get('source', 'Unknown')}, PÃ¡gina {doc.metadata.get('page', 'N/A')}")

if __name__ == "__main__":
    main()
```

---

## Recursos Adicionales

### DocumentaciÃ³n
- [LangChain](https://python.langchain.com/docs/get_started/introduction)
- [Chroma](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)

### Tutoriales
- [RAG from Scratch](https://www.youtube.com/watch?v=LhnCsygAvzY)
- [Advanced RAG Techniques](https://www.youtube.com/watch?v=sVcwVQRHIc8)

### Papers
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

---

**VersiÃ³n**: 1.0  
**Ãšltima actualizaciÃ³n**: Octubre 2025
