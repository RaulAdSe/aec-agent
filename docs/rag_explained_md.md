# üîç RAG (Retrieval Augmented Generation) - Explicaci√≥n T√©cnica

## üìã Tabla de Contenidos

1. [¬øQu√© es RAG?](#qu√©-es-rag)
2. [El Problema que Resuelve](#el-problema-que-resuelve)
3. [Arquitectura de RAG](#arquitectura-de-rag)
4. [Componentes Clave](#componentes-clave)
5. [Pipeline Completo](#pipeline-completo)
6. [Implementaci√≥n con LangChain](#implementaci√≥n-con-langchain)
7. [Embeddings](#embeddings)
8. [Vectorstores](#vectorstores)
9. [Retrieval Strategies](#retrieval-strategies)
10. [Best Practices](#best-practices)
11. [Debugging & Optimization](#debugging--optimization)

---

## ¬øQu√© es RAG?

**RAG = Retrieval Augmented Generation**

Framework que permite a los LLMs acceder a informaci√≥n externa actualizada mediante:
- üìö **Retrieval**: Buscar documentos relevantes
- üß† **Augmentation**: A√±adir contexto al prompt
- ‚úçÔ∏è **Generation**: Generar respuesta informada

### Analog√≠a Simple

Imagina que eres un estudiante haciendo un examen:
- **Sin RAG**: Solo puedes usar lo que memorizaste
- **Con RAG**: Puedes consultar tus apuntes durante el examen

---

## El Problema que Resuelve

### LLMs Tradicionales: Limitaciones

#### Problema 1: Conocimiento Est√°tico

```python
# LLM sin RAG
pregunta = "¬øQu√© dice el CTE DB-SI actualizado en 2023 sobre puertas de evacuaci√≥n?"
respuesta = llm(pregunta)
# ‚ùå Responde con conocimiento antiguo o incorrecto
```

#### Problema 2: Alucinaciones

```python
# Sin RAG, el LLM puede inventar informaci√≥n
pregunta = "¬øCu√°l es el ancho m√≠nimo de puerta seg√∫n el CTE?"
respuesta = llm(pregunta)
# ‚ùå "El ancho m√≠nimo es 75cm" (inventado)
```

#### Problema 3: Sin Citas

```python
# No puedes verificar la fuente
respuesta = "El ancho m√≠nimo es 80cm"
# ‚ùì ¬øDe d√≥nde sale este dato?
```

### RAG: La Soluci√≥n

```python
# Con RAG
pregunta = "¬øQu√© dice el CTE DB-SI sobre puertas de evacuaci√≥n?"

# 1. Buscar en la base de conocimiento
chunks_relevantes = vectorstore.similarity_search(pregunta)
# Encuentra: "CTE DB-SI, Secci√≥n 4.2: El ancho m√≠nimo de puertas de evacuaci√≥n..."

# 2. A√±adir contexto al prompt
contexto = "\n".join(chunks_relevantes)
prompt = f"Contexto: {contexto}\n\nPregunta: {pregunta}"

# 3. Generar respuesta informada
respuesta = llm(prompt)
# ‚úÖ "Seg√∫n el CTE DB-SI Secci√≥n 4.2, el ancho m√≠nimo es 80cm [Fuente: CTE_DB-SI.pdf, p√°g. 23]"
```

---

## Arquitectura de RAG

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FASE 1: INDEXACI√ìN                       ‚îÇ
‚îÇ                    (Se hace una vez)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    üìÑ Documentos (PDFs, TXT, etc.)
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Loader    ‚îÇ  ‚Üê Carga documentos
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Splitter   ‚îÇ  ‚Üê Divide en chunks
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Embeddings  ‚îÇ  ‚Üê Convierte a vectores
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Vectorstore ‚îÇ  ‚Üê Almacena vectores
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASE 2: CONSULTA                          ‚îÇ
‚îÇ                  (Cada vez que preguntas)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    üí¨ Pregunta del usuario
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Embeddings  ‚îÇ  ‚Üê Convierte pregunta a vector
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Retriever  ‚îÇ  ‚Üê Busca chunks similares
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Prompt    ‚îÇ  ‚Üê Construye prompt con contexto
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     LLM     ‚îÇ  ‚Üê Genera respuesta
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚úÖ Respuesta + Citas
```

---

## Componentes Clave

### 1. Document Loader

Carga documentos de diferentes fuentes:

```python
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# PDF
loader = PyPDFLoader("CTE_DB-SI.pdf")
docs = loader.load()

# Texto plano
loader = TextLoader("normativa.txt")
docs = loader.load()
```

**Cada documento tiene**:
- `page_content`: El texto
- `metadata`: Info adicional (nombre archivo, p√°gina, etc.)

### 2. Text Splitter

Divide documentos en chunks manejables:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Tama√±o de cada chunk (caracteres)
    chunk_overlap=200,      # Solapamiento entre chunks
    separators=["\n\n", "\n", " ", ""]  # Prioridad de separaci√≥n
)

chunks = splitter.split_documents(docs)
```

**¬øPor qu√© dividir?**
- Los LLMs tienen l√≠mite de tokens
- Chunks peque√±os = retrieval m√°s preciso
- Overlap = preservar contexto entre chunks

#### Ejemplo de Splitting

```
Documento original:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"El CTE DB-SI establece que las puertas de evacuaci√≥n
deben tener un ancho m√≠nimo de 80cm. En caso de edificios
de p√∫blica concurrencia, este ancho se incrementa a 1.20m.
Las puertas deben abrirse en el sentido de la evacuaci√≥n..."
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Despu√©s del splitting (chunk_size=100, overlap=20):

Chunk 1:
"El CTE DB-SI establece que las puertas de evacuaci√≥n
deben tener un ancho m√≠nimo de 80cm."

Chunk 2 (con overlap):
"ancho m√≠nimo de 80cm. En caso de edificios de p√∫blica
concurrencia, este ancho se incrementa a 1.20m."

Chunk 3:
"ancho se incrementa a 1.20m. Las puertas deben abrirse
en el sentido de la evacuaci√≥n..."
```

### 3. Embeddings

Convierte texto a vectores num√©ricos:

```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Convertir texto a vector
vector = embeddings.embed_query("¬øAncho m√≠nimo de puerta?")
# Output: [0.12, -0.45, 0.89, ..., 0.34]  (384 dimensiones)
```

**Propiedad clave**: Textos similares ‚Üí Vectores cercanos

```python
v1 = embeddings.embed_query("ancho de puerta")
v2 = embeddings.embed_query("anchura de entrada")
v3 = embeddings.embed_query("precio del tomate")

# Distancia coseno:
# similarity(v1, v2) = 0.89  ‚Üê Muy similar
# similarity(v1, v3) = 0.12  ‚Üê Poco similar
```

### 4. Vectorstore

Almacena y busca vectores eficientemente:

```python
from langchain_community.vectorstores import Chroma

# Crear vectorstore
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./vectorstore"
)

# Buscar documentos similares
results = vectorstore.similarity_search(
    "¬øAncho m√≠nimo de puerta?",
    k=3  # Top 3 resultados
)
```

**¬øC√≥mo funciona?**
1. Convierte la pregunta a vector
2. Calcula distancia a todos los vectores almacenados
3. Devuelve los K m√°s cercanos

### 5. Retriever

Interfaz para recuperar documentos:

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",  # O "mmr" para diversidad
    search_kwargs={"k": 3}
)

docs = retriever.get_relevant_documents("¬øAncho m√≠nimo de puerta?")
```

### 6. LLM

Genera la respuesta final:

```python
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-pro",
    temperature=0.1  # Bajo = m√°s determinista
)
```

### 7. Prompt Template

Estructura el prompt con contexto:

```python
from langchain.prompts import ChatPromptTemplate

template = """
Eres un asistente experto en normativa de construcci√≥n.
Responde la pregunta bas√°ndote √öNICAMENTE en el contexto proporcionado.
Si no puedes responder con el contexto dado, di "No tengo informaci√≥n suficiente".

Contexto:
{context}

Pregunta: {question}

Respuesta:
"""

prompt = ChatPromptTemplate.from_template(template)
```

---

## Pipeline Completo

### Implementaci√≥n Paso a Paso

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

# 1. Cargar documentos
loader = PyPDFLoader("CTE_DB-SI.pdf")
documents = loader.load()

# 2. Dividir en chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# 3. Crear embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# 4. Crear vectorstore
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./vectorstore"
)

# 5. Crear retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 6. Crear LLM
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.1)

# 7. Crear cadena QA
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# 8. Hacer pregunta
query = "¬øCu√°l es el ancho m√≠nimo de una puerta de evacuaci√≥n?"
result = qa_chain({"query": query})

print(result["result"])  # Respuesta
print(result["source_documents"])  # Fuentes
```

---

## Implementaci√≥n con LangChain

### Estructura del Proyecto

```
src/rag/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ document_loader.py      # Carga PDFs
‚îú‚îÄ‚îÄ embeddings_config.py    # Configuraci√≥n de embeddings
‚îú‚îÄ‚îÄ vectorstore_manager.py  # Gesti√≥n de vectorstore
‚îî‚îÄ‚îÄ qa_chain.py            # Cadena de QA
```

### document_loader.py

```python
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from typing import List
from langchain.schema import Document

def load_pdfs(pdf_dir: Path) -> List[Document]:
    """
    Carga todos los PDFs de un directorio.
    
    Args:
        pdf_dir: Directorio con PDFs
        
    Returns:
        Lista de documentos cargados
    """
    documents = []
    
    for pdf_path in pdf_dir.glob("*.pdf"):
        print(f"Cargando: {pdf_path.name}")
        loader = PyPDFLoader(str(pdf_path))
        docs = loader.load()
        documents.extend(docs)
    
    print(f"Total documentos cargados: {len(documents)}")
    return documents
```

### embeddings_config.py

```python
from langchain_huggingface import HuggingFaceEmbeddings

def get_embeddings():
    """
    Crea instancia de embeddings.
    Usando modelo multilingual para soportar espa√±ol.
    """
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},  # O 'cuda' si tienes GPU
        encode_kwargs={'normalize_embeddings': True}  # Normalizar vectores
    )
```

### vectorstore_manager.py

```python
from pathlib import Path
from typing import List, Optional
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from .embeddings_config import get_embeddings
from .document_loader import load_pdfs

class VectorstoreManager:
    """Gestiona la creaci√≥n y carga del vectorstore."""
    
    def __init__(self, persist_directory: Path):
        self.persist_directory = persist_directory
        self.embeddings = get_embeddings()
        self.vectorstore: Optional[Chroma] = None
    
    def create_from_pdfs(self, pdf_dir: Path, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        Crea vectorstore desde PDFs.
        
        Args:
            pdf_dir: Directorio con PDFs
            chunk_size: Tama√±o de cada chunk
            chunk_overlap: Solapamiento entre chunks
        """
        # 1. Cargar documentos
        documents = load_pdfs(pdf_dir)
        
        # 2. Dividir en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        print(f"Total chunks: {len(chunks)}")
        
        # 3. Crear vectorstore
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=str(self.persist_directory)
        )
        
        print(f"Vectorstore creado en: {self.persist_directory}")
    
    def load_existing(self):
        """Carga vectorstore existente."""
        self.vectorstore = Chroma(
            persist_directory=str(self.persist_directory),
            embedding_function=self.embeddings
        )
        print(f"Vectorstore cargado desde: {self.persist_directory}")
    
    def get_retriever(self, k: int = 3):
        """
        Obtiene retriever para b√∫squedas.
        
        Args:
            k: N√∫mero de documentos a recuperar
        """
        if not self.vectorstore:
            raise ValueError("Vectorstore no inicializado. Usa create_from_pdfs() o load_existing()")
        
        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )
```

### qa_chain.py

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

SYSTEM_PROMPT = """
Eres un asistente experto en normativa de construcci√≥n espa√±ola (CTE, CTE DB-SI, CTE DB-SUA).
Tu trabajo es responder preguntas bas√°ndote √öNICAMENTE en el contexto proporcionado.

Reglas:
1. Si la informaci√≥n est√° en el contexto, responde de forma clara y precisa
2. Siempre cita la fuente (nombre del documento, secci√≥n, p√°gina si est√° disponible)
3. Si no puedes responder con el contexto dado, di "No tengo informaci√≥n suficiente sobre esto en la normativa proporcionada"
4. No inventes ni supongas informaci√≥n

Contexto:
{context}

Pregunta: {question}

Respuesta detallada:
"""

def create_qa_chain(retriever, model_name: str = "gemini-pro", temperature: float = 0.1):
    """
    Crea cadena de QA con retrieval.
    
    Args:
        retriever: Retriever del vectorstore
        model_name: Nombre del modelo de Google
        temperature: Temperatura del LLM
    
    Returns:
        Cadena de QA configurada
    """
    # LLM
    llm = ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature
    )
    
    # Prompt
    prompt = PromptTemplate(
        template=SYSTEM_PROMPT,
        input_variables=["context", "question"]
    )
    
    # Cadena
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # Estrategia: meter todo el contexto
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    
    return qa_chain

def query(qa_chain, question: str):
    """
    Hace una pregunta a la cadena de QA.
    
    Args:
        qa_chain: Cadena de QA
        question: Pregunta del usuario
    
    Returns:
        Dict con 'result' y 'source_documents'
    """
    result = qa_chain({"query": question})
    return result
```

---

## Embeddings

### ¬øQu√© son?

Representaciones num√©ricas de texto que capturan significado sem√°ntico.

```
"ancho de puerta" ‚Üí [0.12, -0.45, 0.89, ..., 0.34]
                     ‚Üë
                  Vector de 384 dimensiones
```

### Tipos de Embeddings

#### 1. Word Embeddings (Antiguo)

Cada palabra tiene un vector fijo:

```python
# Word2Vec
"puerta" ‚Üí [0.1, 0.5, ...]
"door" ‚Üí [0.2, 0.6, ...]
```

**Problema**: "banco" (asiento) y "banco" (instituci√≥n) tienen el mismo vector.

#### 2. Sentence Embeddings (Moderno)

Toda la frase tiene un vector:

```python
"ancho de puerta de evacuaci√≥n" ‚Üí [0.12, -0.45, ..., 0.34]
"width of evacuation door" ‚Üí [0.15, -0.42, ..., 0.31]  # Similar
```

**Ventaja**: Captura contexto completo.

### Modelos Recomendados

#### Para Espa√±ol/Multiling√ºe

```python
# Opci√≥n 1: Peque√±o y r√°pido (recomendado)
"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# - Tama√±o: 420 MB
# - Dimensiones: 384
# - Idiomas: 50+

# Opci√≥n 2: M√°s preciso pero m√°s pesado
"sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# - Tama√±o: 1.1 GB
# - Dimensiones: 768
# - Idiomas: 50+
```

#### Para Solo Ingl√©s

```python
# Mejor rendimiento
"sentence-transformers/all-MiniLM-L6-v2"
# - Tama√±o: 80 MB
# - Dimensiones: 384
```

### Uso en C√≥digo

```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Embeddings para una query
query_vector = embeddings.embed_query("¬øAncho m√≠nimo de puerta?")
print(f"Dimensiones: {len(query_vector)}")  # 384

# Embeddings para m√∫ltiples documentos
docs = ["doc1", "doc2", "doc3"]
doc_vectors = embeddings.embed_documents(docs)
print(f"N√∫mero de vectores: {len(doc_vectors)}")  # 3
```

---

## Vectorstores

### ¬øQu√© es un Vectorstore?

Base de datos especializada en:
1. Almacenar vectores de alta dimensionalidad
2. Buscar vectores similares eficientemente

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         VECTORSTORE                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ID  ‚îÇ Vector          ‚îÇ Metadata      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1   ‚îÇ [0.1, 0.5, ...] ‚îÇ {page: 1,...} ‚îÇ
‚îÇ 2   ‚îÇ [0.2, 0.6, ...] ‚îÇ {page: 2,...} ‚îÇ
‚îÇ 3   ‚îÇ [0.3, 0.4, ...] ‚îÇ {page: 3,...} ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Opciones Populares

#### ChromaDB (Recomendado para POC)

```python
from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./vectorstore"  # Persiste en disco
)
```

**Pros**:
- ‚úÖ Simple de usar
- ‚úÖ Funciona en local (no requiere servidor)
- ‚úÖ Persiste autom√°ticamente
- ‚úÖ Bueno para desarrollo

**Cons**:
- ‚ùå No escalable para producci√≥n (millones de vectores)

#### Pinecone (Producci√≥n)

```python
from langchain_community.vectorstores import Pinecone

vectorstore = Pinecone.from_documents(
    documents=chunks,
    embedding=embeddings,
    index_name="normativa"
)
```

**Pros**:
- ‚úÖ Muy escalable
- ‚úÖ Servicio gestionado
- ‚úÖ R√°pido en b√∫squedas

**Cons**:
- ‚ùå Requiere cuenta y pago
- ‚ùå M√°s complejo de configurar

#### FAISS (Local, Escalable)

```python
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)
vectorstore.save_local("./vectorstore")
```

**Pros**:
- ‚úÖ Muy r√°pido
- ‚úÖ Funciona en local
- ‚úÖ Escalable a millones de vectores

**Cons**:
- ‚ùå M√°s complejo que ChromaDB
- ‚ùå Requiere m√°s memoria RAM

### B√∫squeda de Similitud

#### Distancia Coseno (Por Defecto)

Mide el √°ngulo entre vectores:

```python
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(v1, v2):
    return dot(v1, v2) / (norm(v1) * norm(v2))

# Rango: [-1, 1]
# 1 = id√©nticos
# 0 = ortogonales
# -1 = opuestos
```

#### Distancia Euclidiana

Mide la distancia "real" entre vectores:

```python
from numpy import sqrt, sum

def euclidean_distance(v1, v2):
    return sqrt(sum((v1 - v2) ** 2))

# Rango: [0, ‚àû)
# 0 = id√©nticos
# Mayor valor = m√°s diferentes
```

---

## Retrieval Strategies

### üöÄ Sistema Unificado de Estrategias

El sistema RAG implementado incluye **5 estrategias de recuperaci√≥n** que puedes cambiar en tiempo real:

#### 1. **Basic Retrieval** (B√∫squeda por Palabras Clave)

```python
# Estrategia b√°sica - solo coincidencias de palabras
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

docs = retriever.get_relevant_documents("¬øAncho m√≠nimo de puerta?")
# Devuelve los 3 chunks con m√°s coincidencias de palabras
```

**Caracter√≠sticas**:
- ‚úÖ R√°pida y simple
- ‚úÖ No requiere embeddings
- ‚ùå Solo busca coincidencias exactas de palabras
- ‚ùå No entiende el significado sem√°ntico

#### 2. **Semantic Retrieval** (B√∫squeda Sem√°ntica)

```python
# Usando embeddings para entender significado
from sentence_transformers import SentenceTransformer

embeddings_model = SentenceTransformer(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# Convierte pregunta a vector y busca similares
query_embedding = embeddings_model.encode(["¬øAncho m√≠nimo de puerta?"])
similarities = cosine_similarity(query_embedding, chunk_embeddings)
```

**Caracter√≠sticas**:
- ‚úÖ Entiende el significado de las consultas
- ‚úÖ Encuentra contenido relacionado conceptualmente
- ‚úÖ Mejor para consultas complejas
- ‚ùå Requiere m√°s recursos computacionales
- ‚ùå Puede perder coincidencias exactas importantes

#### 3. **Keyword Retrieval** (B√∫squeda Avanzada por Palabras)

```python
# TF-IDF-like scoring con m√∫ltiples se√±ales
def keyword_search(query, chunks):
    query_words = set(query.lower().split())
    scored_chunks = []
    
    for chunk in chunks:
        chunk_words = set(chunk['text'].lower().split())
        
        # M√∫ltiples puntuaciones
        word_overlap = len(query_words.intersection(chunk_words))
        word_density = word_overlap / len(query_words)
        chunk_word_count = len(chunk_words)
        
        # Puntuaci√≥n combinada
        score = word_overlap * 0.5 + word_density * 0.3 + (1 / (1 + chunk_word_count)) * 0.2
        scored_chunks.append((chunk, score))
    
    return sorted(scored_chunks, key=lambda x: x[1], reverse=True)
```

**Caracter√≠sticas**:
- ‚úÖ Mejor que b√∫squeda b√°sica
- ‚úÖ Considera densidad de palabras y longitud
- ‚úÖ No requiere embeddings
- ‚ùå A√∫n limitado a coincidencias exactas

#### 4. **Hybrid Retrieval** (B√∫squeda H√≠brida)

```python
# Combina sem√°ntica (70%) + keyword (30%)
def hybrid_search(query, chunks, semantic_weight=0.7):
    # Obtener resultados sem√°nticos
    semantic_results = semantic_search(query, chunks)
    semantic_scores = {chunk['text']: score for chunk, score in semantic_results}
    
    # Obtener resultados por palabras clave
    keyword_results = keyword_search(query, chunks)
    keyword_scores = {chunk['text']: score for chunk, score in keyword_results}
    
    # Combinar puntuaciones
    combined_results = []
    for chunk_text in set(semantic_scores.keys()) | set(keyword_scores.keys()):
        semantic_score = semantic_scores.get(chunk_text, 0)
        keyword_score = keyword_scores.get(chunk_text, 0)
        
        # Puntuaci√≥n combinada
        combined_score = (semantic_weight * semantic_score + 
                         (1 - semantic_weight) * keyword_score)
        combined_results.append((chunk_obj, combined_score))
    
    return sorted(combined_results, key=lambda x: x[1], reverse=True)
```

**Caracter√≠sticas**:
- ‚úÖ Combina lo mejor de ambas estrategias
- ‚úÖ 70% sem√°ntico + 30% keyword para balance √≥ptimo
- ‚úÖ Mejor precisi√≥n y recall
- ‚úÖ Recomendada para producci√≥n

#### 5. **Rerank Retrieval** (B√∫squeda con Reranking)

```python
# H√≠brida + reranking con se√±ales adicionales
def rerank_results(query, results):
    reranked = []
    
    for chunk, original_score in results:
        # Calcular se√±ales de relevancia adicionales
        relevance_score = calculate_relevance_score(query, chunk)
        
        # Combinar puntuaci√≥n original con relevancia
        final_score = original_score * 0.6 + relevance_score * 0.4
        reranked.append((chunk, final_score))
    
    return sorted(reranked, key=lambda x: x[1], reverse=True)

def calculate_relevance_score(query, chunk):
    text = chunk['text'].lower()
    query_lower = query.lower()
    
    score = 0.0
    
    # Coincidencia exacta de frase
    if query_lower in text:
        score += 0.3
    
    # Densidad de palabras de la consulta
    query_words = query_lower.split()
    chunk_words = text.split()
    word_matches = sum(1 for word in query_words if word in chunk_words)
    word_density = word_matches / len(query_words) if query_words else 0
    score += word_density * 0.2
    
    # Penalizaci√≥n por longitud (preferir chunks de longitud media)
    length_penalty = 1.0 - abs(len(chunk['text']) - 800) / 1000
    score += max(0, length_penalty) * 0.1
    
    # Bonus por posici√≥n de p√°gina (p√°ginas tempranas m√°s importantes)
    if chunk['metadata']['pages']:
        page_bonus = 1.0 - (min(chunk['metadata']['pages']) / 100)
        score += max(0, page_bonus) * 0.1
    
    return min(1.0, score)
```

**Caracter√≠sticas**:
- ‚úÖ La estrategia m√°s avanzada
- ‚úÖ Considera m√∫ltiples se√±ales de relevancia
- ‚úÖ Optimiza longitud de chunks y posici√≥n de p√°gina
- ‚úÖ Mejor rendimiento general

### üîÑ Cambio de Estrategias en Tiempo Real

```python
from scripts.unified_rag_system import UnifiedRAGSystem, RetrievalStrategy

# Inicializar con estrategia espec√≠fica
rag = UnifiedRAGSystem(RetrievalStrategy.BASIC)

# Cambiar estrategia sin recargar documentos
rag.switch_strategy(RetrievalStrategy.SEMANTIC)
rag.switch_strategy(RetrievalStrategy.HYBRID)
rag.switch_strategy(RetrievalStrategy.RERANK)
```

### üìä Comparaci√≥n de Estrategias

| Estrategia | Velocidad | Precisi√≥n | Recursos | Uso Recomendado |
|------------|-----------|-----------|----------|-----------------|
| **Basic** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | üü¢ Bajo | Desarrollo r√°pido |
| **Semantic** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | üü° Medio | Consultas complejas |
| **Keyword** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | üü¢ Bajo | B√∫squedas exactas |
| **Hybrid** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üü° Medio | **Producci√≥n** |
| **Rerank** | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üî¥ Alto | **M√°xima calidad** |

### üõ†Ô∏è Estrategias Tradicionales (LangChain)

#### MMR (Maximal Marginal Relevance)

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 3,
        "fetch_k": 20,  # Candidatos a considerar
        "lambda_mult": 0.5  # 0 = diversidad, 1 = relevancia
    }
)
```

**¬øCu√°ndo usar MMR?**
- ‚úÖ Cuando quieres perspectivas diferentes
- ‚úÖ Para evitar chunks muy repetitivos
- ‚ùå No uses si necesitas m√°xima precisi√≥n

#### Threshold-based

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.8,  # M√≠nimo 80% de similitud
        "k": 5
    }
)
```

---

## üöÄ Sistema Unificado RAG

### Scripts Disponibles

El proyecto incluye m√∫ltiples scripts para diferentes casos de uso:

#### 1. **Sistema Unificado** (`unified_rag_system.py`)

```bash
# Usar con estrategia espec√≠fica
python scripts/unified_rag_system.py --strategy basic
python scripts/unified_rag_system.py --strategy semantic
python scripts/unified_rag_system.py --strategy hybrid
python scripts/unified_rag_system.py --strategy rerank

# Modo interactivo con cambio de estrategias
python scripts/unified_rag_system.py --strategy basic
# Luego en el prompt:
‚ùì [basic] Tu consulta: switch semantic
üîÑ Switching from basic to semantic
‚úÖ Switched to semantic strategy
```

#### 2. **Comparaci√≥n de Estrategias** (`retrieval_comparison.py`)

```bash
# Compara todas las estrategias lado a lado
python scripts/retrieval_comparison.py
```

**Salida esperada**:
```
üîç Query: '¬øCu√°l es el ancho m√≠nimo de una puerta de evacuaci√≥n?'

1Ô∏è‚É£ BASIC RETRIEVAL (Keyword matching):
   1. Score: 5.000 | P√°ginas: [23]
      Texto: "El ancho m√≠nimo de puertas de evacuaci√≥n..."

2Ô∏è‚É£ SEMANTIC RETRIEVAL (Embeddings):
   1. Score: 0.755 | P√°ginas: [23]
      Texto: "CTE DB-SI establece que las puertas..."

3Ô∏è‚É£ HYBRID RETRIEVAL (Semantic + Keyword):
   1. Score: 2.029 | P√°ginas: [23]
      Texto: "El ancho m√≠nimo de puertas de evacuaci√≥n..."
```

#### 3. **Demo Avanzado** (`advanced_rag_demo.py`)

```bash
# Demo completo con todas las funcionalidades
python scripts/advanced_rag_demo.py
```

#### 4. **Test B√°sico** (`simple_rag_test.py`)

```bash
# Verificaci√≥n r√°pida del sistema
python scripts/simple_rag_test.py
```

### Uso Program√°tico

#### Inicializaci√≥n B√°sica

```python
from scripts.unified_rag_system import UnifiedRAGSystem, RetrievalStrategy
from pathlib import Path

# Crear sistema RAG
rag = UnifiedRAGSystem(RetrievalStrategy.HYBRID)

# Cargar documentos
rag.load_and_chunk_pdf(Path("data/normativa/DBSI.pdf"))

# Hacer consulta
results = rag.retrieve("¬øCu√°l es el ancho m√≠nimo de una puerta de evacuaci√≥n?", top_k=3)

# Mostrar resultados
for i, (chunk, score) in enumerate(results, 1):
    print(f"{i}. Score: {score:.3f}")
    print(f"   P√°ginas: {chunk['metadata']['pages']}")
    print(f"   Texto: {chunk['text'][:100]}...")
```

#### Cambio Din√°mico de Estrategias

```python
# Inicializar con estrategia b√°sica
rag = UnifiedRAGSystem(RetrievalStrategy.BASIC)
rag.load_and_chunk_pdf(Path("data/normativa/DBSI.pdf"))

# Probar diferentes estrategias para la misma consulta
query = "¬øQu√© dice sobre distancias de evacuaci√≥n?"

print("=== BASIC ===")
basic_results = rag.retrieve(query, top_k=2)
for chunk, score in basic_results:
    print(f"Score: {score:.3f} | {chunk['text'][:50]}...")

print("\n=== SEMANTIC ===")
rag.switch_strategy(RetrievalStrategy.SEMANTIC)
semantic_results = rag.retrieve(query, top_k=2)
for chunk, score in semantic_results:
    print(f"Score: {score:.3f} | {chunk['text'][:50]}...")

print("\n=== HYBRID ===")
rag.switch_strategy(RetrievalStrategy.HYBRID)
hybrid_results = rag.retrieve(query, top_k=2)
for chunk, score in hybrid_results:
    print(f"Score: {score:.3f} | {chunk['text'][:50]}...")
```

#### Integraci√≥n con OpenAI

```python
import openai
from scripts.unified_rag_system import UnifiedRAGSystem, RetrievalStrategy

# Configurar OpenAI
openai.api_key = "tu-api-key"

# Sistema RAG
rag = UnifiedRAGSystem(RetrievalStrategy.RERANK)
rag.load_and_chunk_pdf(Path("data/normativa/DBSI.pdf"))

def query_with_openai(question: str) -> str:
    # Recuperar contexto relevante
    results = rag.retrieve(question, top_k=3)
    context = "\n\n".join([chunk['text'] for chunk, score in results])
    
    # Generar respuesta con OpenAI
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "Eres un experto en normativa de construcci√≥n espa√±ola."},
            {"role": "user", "content": f"Contexto: {context}\n\nPregunta: {question}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content

# Usar
answer = query_with_openai("¬øCu√°l es el ancho m√≠nimo de una puerta de evacuaci√≥n?")
print(answer)
```

### Configuraci√≥n Avanzada

#### Personalizar Par√°metros

```python
# Crear sistema con par√°metros personalizados
rag = UnifiedRAGSystem(RetrievalStrategy.HYBRID)

# Cargar con chunking personalizado
rag.load_and_chunk_pdf(
    Path("data/normativa/DBSI.pdf"),
    chunk_size=1500,  # Chunks m√°s grandes
    overlap=300       # M√°s solapamiento
)

# Recuperar con m√°s resultados
results = rag.retrieve("pregunta", top_k=5)
```

#### Manejo de Errores

```python
try:
    rag = UnifiedRAGSystem(RetrievalStrategy.SEMANTIC)
    rag.load_and_chunk_pdf(Path("data/normativa/DBSI.pdf"))
    
    results = rag.retrieve("pregunta", top_k=3)
    
except FileNotFoundError:
    print("‚ùå Archivo PDF no encontrado")
except ImportError:
    print("‚ùå Dependencias faltantes. Instala: pip install sentence-transformers")
except Exception as e:
    print(f"‚ùå Error: {e}")
```

### Rendimiento y Optimizaci√≥n

#### Benchmarking de Estrategias

```python
import time
from scripts.unified_rag_system import UnifiedRAGSystem, RetrievalStrategy

rag = UnifiedRAGSystem(RetrievalStrategy.BASIC)
rag.load_and_chunk_pdf(Path("data/normativa/DBSI.pdf"))

query = "¬øCu√°l es el ancho m√≠nimo de una puerta de evacuaci√≥n?"

strategies = [
    RetrievalStrategy.BASIC,
    RetrievalStrategy.SEMANTIC,
    RetrievalStrategy.KEYWORD,
    RetrievalStrategy.HYBRID,
    RetrievalStrategy.RERANK
]

for strategy in strategies:
    rag.switch_strategy(strategy)
    
    start_time = time.time()
    results = rag.retrieve(query, top_k=3)
    end_time = time.time()
    
    print(f"{strategy.value:10} | {end_time - start_time:.3f}s | Score: {results[0][1]:.3f}")
```

**Salida esperada**:
```
basic      | 0.001s | Score: 5.000
semantic   | 0.045s | Score: 0.755
keyword    | 0.002s | Score: 3.733
hybrid     | 0.047s | Score: 1.369
rerank     | 0.048s | Score: 0.925
```

---

## Best Practices

### 1. Chunk Size √ìptimo

```python
# Para documentaci√≥n t√©cnica
chunk_size = 1000  # 1000 caracteres ‚âà 250 tokens
chunk_overlap = 200  # 20% overlap

# Para preguntas cortas
chunk_size = 500
chunk_overlap = 100

# Para contextos largos
chunk_size = 1500
chunk_overlap = 300
```

**Regla general**: 
- Chunks peque√±os ‚Üí B√∫squeda precisa, pero puede perder contexto
- Chunks grandes ‚Üí M√°s contexto, pero menos preciso

### 2. Metadata

A√±ade metadata √∫til a cada chunk:

```python
from langchain.schema import Document

doc = Document(
    page_content="El ancho m√≠nimo es 80cm",
    metadata={
        "source": "CTE_DB-SI.pdf",
        "page": 23,
        "section": "4.2 Puertas de evacuaci√≥n",
        "chapter": "Evacuaci√≥n"
    }
)
```

Luego puedes filtrar por metadata:

```python
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 3,
        "filter": {"section": "4.2 Puertas de evacuaci√≥n"}
    }
)
```

### 3. Prompt Engineering

**Mal prompt**:
```python
"Responde la pregunta: {question}"
```

**Buen prompt**:
```python
"""
Eres un experto en normativa de construcci√≥n.
Responde bas√°ndote √öNICAMENTE en el contexto.
Cita siempre la fuente.

Contexto: {context}
Pregunta: {question}
Respuesta:
"""
```

### 4. Temperatura del LLM

```python
# Para respuestas t√©cnicas/precisas
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.1)

# Para respuestas creativas
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.7)
```

### 5. Evaluaci√≥n

Mide la calidad de tu RAG:

```python
from langchain.evaluation.qa import QAEvalChain

# Pares de pregunta-respuesta esperada
examples = [
    {
        "query": "¬øAncho m√≠nimo de puerta?",
        "answer": "80cm seg√∫n CTE DB-SI"
    },
    # ...
]

# Evaluar
predictions = [qa_chain(ex["query"]) for ex in examples]
eval_chain = QAEvalChain.from_llm(llm)
graded_outputs = eval_chain.evaluate(examples, predictions)
```

---

## Debugging & Optimization

### Problema 1: Respuestas Incorrectas

**Diagn√≥stico**:
```python
result = qa_chain({"query": "¬øAncho m√≠nimo de puerta?"})

# 1. Revisar chunks recuperados
for doc in result["source_documents"]:
    print(doc.page_content)
    print(doc.metadata)
    print("---")
```

**Posibles causas**:
- ‚ùå Chunks no contienen la informaci√≥n
- ‚ùå K demasiado bajo (aumenta a 5-7)
- ‚ùå Embeddings no son buenos para espa√±ol

**Soluci√≥n**:
```python
# Aumentar K
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Cambiar modelo de embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"  # Mejor
)
```

### Problema 2: Contexto Insuficiente

**Soluci√≥n**: Aumentar chunk_size y overlap

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # Era 1000
    chunk_overlap=300  # Era 200
)
```

### Problema 3: Lento

**Diagn√≥stico**:
```python
import time

start = time.time()
result = qa_chain({"query": "pregunta"})
print(f"Tiempo: {time.time() - start:.2f}s")
```

**Optimizaciones**:

1. **Usar GPU para embeddings**:
```python
embeddings = HuggingFaceEmbeddings(
    model_kwargs={'device': 'cuda'}  # Era 'cpu'
)
```

2. **Reducir K**:
```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})  # Era 5
```

3. **Usar modelo de embeddings m√°s peque√±o**:
```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"  # M√°s r√°pido
)
```

### Problema 4: Vectorstore Muy Grande

**Soluci√≥n**: Limpieza y optimizaci√≥n

```python
# Solo indexar PDFs relevantes
pdfs_to_index = [
    "CTE_DB-SI.pdf",
    "CTE_DB-SUA.pdf"
]

documents = []
for pdf in pdfs_to_index:
    loader = PyPDFLoader(pdf)
    documents.extend(loader.load())
```

---

## Ejemplo Completo

```python
# main.py
from pathlib import Path
from src.rag.vectorstore_manager import VectorstoreManager
from src.rag.qa_chain import create_qa_chain, query

def main():
    # Configuraci√≥n
    PDF_DIR = Path("data/normativa")
    VECTORSTORE_DIR = Path("vectorstore/normativa_db")
    
    # Crear o cargar vectorstore
    rag = VectorstoreManager(VECTORSTORE_DIR)
    
    if not VECTORSTORE_DIR.exists():
        print("Creando vectorstore por primera vez...")
        rag.create_from_pdfs(PDF_DIR)
    else:
        print("Cargando vectorstore existente...")
        rag.load_existing()
    
    # Crear retriever
    retriever = rag.get_retriever(k=3)
    
    # Crear cadena de QA
    qa_chain = create_qa_chain(retriever)
    
    # Hacer preguntas
    questions = [
        "¬øCu√°l es el ancho m√≠nimo de una puerta de evacuaci√≥n?",
        "¬øQu√© dice el CTE sobre distancias m√°ximas de evacuaci√≥n?",
        "¬øRequisitos de resistencia al fuego para muros?"
    ]
    
    for question in questions:
        print(f"\nPregunta: {question}")
        result = query(qa_chain, question)
        print(f"Respuesta: {result['result']}")
        print("\nFuentes:")
        for doc in result['source_documents']:
            print(f"  - {doc.metadata.get('source', 'Unknown')}, P√°gina {doc.metadata.get('page', 'N/A')}")

if __name__ == "__main__":
    main()
```

---

## Recursos Adicionales

### Documentaci√≥n
- [LangChain](https://python.langchain.com/docs/get_started/introduction)
- [Chroma](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)

### Tutoriales
- [RAG from Scratch](https://www.youtube.com/watch?v=LhnCsygAvzY)
- [Advanced RAG Techniques](https://www.youtube.com/watch?v=sVcwVQRHIc8)

### Papers
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

---

---

## üìã Resumen del Sistema RAG Implementado

### ‚úÖ Caracter√≠sticas Implementadas

- **5 Estrategias de Recuperaci√≥n**: Basic, Semantic, Keyword, Hybrid, Rerank
- **Cambio en Tiempo Real**: Switch entre estrategias sin recargar documentos
- **Sistema Unificado**: Una interfaz para todas las estrategias
- **Scripts de Demostraci√≥n**: M√∫ltiples herramientas de testing y comparaci√≥n
- **Integraci√≥n OpenAI**: Compatible con gpt-3.5-turbo, gpt-4, gpt-5-nano
- **Embeddings Multiling√ºes**: Optimizado para espa√±ol
- **Reranking Avanzado**: M√∫ltiples se√±ales de relevancia
- **Documentaci√≥n Completa**: Gu√≠as de uso y ejemplos

### üöÄ Scripts Disponibles

| Script | Prop√≥sito | Uso Recomendado |
|--------|-----------|-----------------|
| `unified_rag_system.py` | Sistema principal con cambio de estrategias | **Producci√≥n** |
| `retrieval_comparison.py` | Comparaci√≥n lado a lado de estrategias | **An√°lisis** |
| `advanced_rag_demo.py` | Demo completo con todas las funcionalidades | **Demostraci√≥n** |
| `simple_rag_test.py` | Test b√°sico de funcionalidad | **Verificaci√≥n** |
| `working_rag_demo.py` | Demo simplificado sin LangChain | **Fallback** |

### üéØ Recomendaciones de Uso

#### Para Desarrollo
```bash
# Test r√°pido
python scripts/simple_rag_test.py

# Comparar estrategias
python scripts/retrieval_comparison.py
```

#### Para Producci√≥n
```bash
# Sistema unificado con estrategia h√≠brida
python scripts/unified_rag_system.py --strategy hybrid

# O con reranking para m√°xima calidad
python scripts/unified_rag_system.py --strategy rerank
```

#### Para An√°lisis
```bash
# Demo completo
python scripts/advanced_rag_demo.py
```

### üìä Rendimiento por Estrategia

| Estrategia | Velocidad | Precisi√≥n | Recursos | Caso de Uso |
|------------|-----------|-----------|----------|-------------|
| **Basic** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | üü¢ Bajo | Desarrollo r√°pido |
| **Semantic** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | üü° Medio | Consultas complejas |
| **Keyword** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | üü¢ Bajo | B√∫squedas exactas |
| **Hybrid** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üü° Medio | **Producci√≥n** |
| **Rerank** | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üî¥ Alto | **M√°xima calidad** |

### üîß Configuraci√≥n M√≠nima

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Configurar API key
export OPENAI_API_KEY="tu-api-key"

# 3. Probar sistema
python scripts/simple_rag_test.py

# 4. Usar sistema unificado
python scripts/unified_rag_system.py --strategy hybrid
```

### üìÅ Estructura de Archivos

```
scripts/
‚îú‚îÄ‚îÄ unified_rag_system.py      # Sistema principal
‚îú‚îÄ‚îÄ retrieval_comparison.py    # Comparaci√≥n de estrategias
‚îú‚îÄ‚îÄ advanced_rag_demo.py       # Demo avanzado
‚îú‚îÄ‚îÄ simple_rag_test.py         # Test b√°sico
‚îî‚îÄ‚îÄ working_rag_demo.py        # Demo simplificado

src/rag/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ document_loader.py         # Carga de PDFs
‚îú‚îÄ‚îÄ embeddings_config.py       # Configuraci√≥n embeddings
‚îú‚îÄ‚îÄ vectorstore_manager.py     # Gesti√≥n vectorstore
‚îî‚îÄ‚îÄ qa_chain.py               # Cadena de QA

docs/
‚îî‚îÄ‚îÄ rag_explained_md.md       # Esta documentaci√≥n
```

---

**Versi√≥n**: 2.0  
**√öltima actualizaci√≥n**: Diciembre 2024  
**Estado**: ‚úÖ Sistema completo y funcional
