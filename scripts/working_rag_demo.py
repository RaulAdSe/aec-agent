#!/usr/bin/env python3
"""
Working RAG Demo - Simplified version that works with current dependencies.

This script demonstrates the RAG system functionality using direct OpenAI API calls
and basic document processing without complex LangChain dependencies.
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import openai
from pypdf import PdfReader

# Load environment variables
load_dotenv()

def load_pdf_content(pdf_path: Path) -> str:
    """Load and extract text from PDF."""
    print(f"üìÑ Loading PDF: {pdf_path.name}")
    
    try:
        reader = PdfReader(pdf_path)
        text = ""
        
        for page_num, page in enumerate(reader.pages):
            page_text = page.extract_text()
            text += f"\n--- P√°gina {page_num + 1} ---\n"
            text += page_text
        
        print(f"‚úÖ Loaded {len(reader.pages)} pages, {len(text)} characters")
        return text
        
    except Exception as e:
        print(f"‚ùå Error loading PDF: {e}")
        return ""

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> list:
    """Split text into chunks for processing."""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # Try to break at sentence boundary
        if end < len(text):
            last_period = chunk.rfind('.')
            if last_period > start + chunk_size // 2:
                end = start + last_period + 1
                chunk = text[start:end]
        
        chunks.append(chunk.strip())
        start = end - overlap
    
    print(f"üìù Created {len(chunks)} text chunks")
    return chunks

def find_relevant_chunks(query: str, chunks: list, top_k: int = 3) -> list:
    """Find most relevant chunks using simple keyword matching."""
    query_words = set(query.lower().split())
    scored_chunks = []
    
    for i, chunk in enumerate(chunks):
        chunk_words = set(chunk.lower().split())
        # Simple scoring based on word overlap
        score = len(query_words.intersection(chunk_words))
        scored_chunks.append((score, i, chunk))
    
    # Sort by score and return top chunks
    scored_chunks.sort(reverse=True)
    return [chunk for score, i, chunk in scored_chunks[:top_k]]

def query_openai(context: str, question: str, model: str = None) -> str:
    """Query OpenAI with context and question."""
    if model is None:
        model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
    
    system_prompt = """Eres un asistente experto en normativa de construcci√≥n espa√±ola, especializado en el C√≥digo T√©cnico de la Edificaci√≥n (CTE), especialmente en el Documento B√°sico de Seguridad en caso de Incendio (DB-SI).

Tu trabajo es responder preguntas bas√°ndote √öNICAMENTE en el contexto proporcionado de la normativa.

REGLAS IMPORTANTES:
1. Responde SIEMPRE en espa√±ol
2. Si la informaci√≥n est√° en el contexto, responde de forma clara y precisa
3. SIEMPRE cita la fuente espec√≠fica (documento, secci√≥n, p√°gina si est√° disponible)
4. Si no puedes responder con el contexto dado, di claramente "No tengo informaci√≥n suficiente sobre esto en la normativa proporcionada"
5. NO inventes ni supongas informaci√≥n que no est√© en el contexto
6. Si hay m√∫ltiples requisitos, enum√©ralos claramente
7. Incluye valores num√©ricos espec√≠ficos cuando est√©n disponibles"""

    user_prompt = f"""CONTEXTO:
{context}

PREGUNTA: {question}

RESPUESTA DETALLADA:"""

    try:
        # Use model-specific parameters
        if "gpt-5" in model:
            response = openai.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
        elif "gpt-4" in model:
            response = openai.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_completion_tokens=1000
            )
        else:
            response = openai.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return f"Error al consultar OpenAI: {e}"

def main():
    """Main demo function."""
    print("üöÄ AEC Compliance Agent - Working RAG Demo")
    print("=" * 50)
    
    # Check environment
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("‚ùå OPENAI_API_KEY not found!")
        return 1
    
    # Set OpenAI API key
    openai.api_key = openai_key
    
    # Load PDF
    pdf_path = Path("data/normativa/DBSI.pdf")
    if not pdf_path.exists():
        print(f"‚ùå PDF not found: {pdf_path}")
        return 1
    
    # Load and chunk PDF content
    print("\nüìö Processing PDF document...")
    pdf_text = load_pdf_content(pdf_path)
    if not pdf_text:
        print("‚ùå Failed to load PDF content")
        return 1
    
    chunks = chunk_text(pdf_text, chunk_size=1500, overlap=300)
    
    # Test questions
    test_questions = [
        "¬øCu√°l es el ancho m√≠nimo de una puerta de evacuaci√≥n?",
        "¬øQu√© dice el CTE DB-SI sobre las distancias m√°ximas de evacuaci√≥n?",
        "¬øCu√°les son los requisitos de resistencia al fuego para muros?",
        "¬øQu√© tipos de sistemas de detecci√≥n de incendios se mencionan?",
        "¬øCu√°les son las condiciones para la sectorizaci√≥n de incendios?"
    ]
    
    print(f"\nüß™ Testing RAG system with {len(test_questions)} questions...")
    print("=" * 50)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüìù Pregunta {i}: {question}")
        print("-" * 40)
        
        # Find relevant chunks
        relevant_chunks = find_relevant_chunks(question, chunks, top_k=3)
        context = "\n\n".join(relevant_chunks)
        
        # Query OpenAI
        print("ü§î Procesando...")
        answer = query_openai(context, question)
        
        print(f"üí° Respuesta:\n{answer}")
        print("\n" + "="*50)
    
    # Interactive mode
    print("\nüí¨ Modo Interactivo (escribe 'quit' para salir)")
    while True:
        try:
            user_question = input("\n‚ùì Tu pregunta: ").strip()
            
            if user_question.lower() in ['quit', 'exit', 'salir']:
                break
            
            if not user_question:
                continue
            
            # Find relevant chunks
            relevant_chunks = find_relevant_chunks(user_question, chunks, top_k=3)
            context = "\n\n".join(relevant_chunks)
            
            # Query OpenAI
            print("ü§î Procesando...")
            answer = query_openai(context, user_question)
            
            print(f"\nüí° Respuesta:\n{answer}")
            
        except KeyboardInterrupt:
            print("\nüëã ¬°Hasta luego!")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")
    
    return 0

if __name__ == "__main__":
    exit(main())
